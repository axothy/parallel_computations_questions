# Параллельные вычисления. Вопросы к экзамену 2024

<!--Writerside adds this topic when you create a new documentation project.
You can use it as a sandbox to play with Writerside features, and remove it from the TOC when you don't need it anymore.-->

## 1. Типы параллелизма. Классификации Флинна, Хокни, Фенга, Хендлера, Скилликорна.

Начнем с самого нижнего

![image.png](image.png)

Тиражирование количества логических схем по числу разрядов, 
чтобы могли одновременно могли выполнять операцию суммирования большим числом 
разрядов. Самый простой и понятный путь. Но тут есть предел.

![image_1.png](image_1.png)

Самое традиционное - **конвейеризация**, например конвейеризация команд процессора, разделяя на микрокоманды которые можно выполнять в конвейере. Тактовая частота повышается. Но есть ограничения.

**Суперскалярность** - современные процессоры верхнего уровня не просто выполняют команду как последовательность, а разбивается команда на микрокоманды и эти микрокоманды исполняются и диспетчеризуются в параллельной системе, чтобы этот процессор мог в один момент времени в один такт выполнить больше одной команды.

Одно из последних достижений - **явный параллелизм команд** с большой длиной слова. Vliw и epic - здесь подход состоит в том что мы отправляем команды процессора не по одиночке, а целыми бандлами (пачками), мы знаем какие команды можно выполнить параллельно, и формируем длинное командное слово. И отправляем процессору, процессор их исполняет максимально параллельно. Задача распараллеливания делегируется компилятору.

![image_2.png](image_2.png)

Это макропараллелизм - это напрямую ложится на программиста. 
Целевую задачу разбиваем на куски и выполняем на нескольких процессорах. 

**Параллелизм задач** - мы логически разбиваем задачу на отдельные куски/этапы, 
и будет макроконвейер. Разбить задачу на логические части - тут есть предел дробления.

**Параллелизм данных** - когда решаем одну и ту же задачу но на больших данных, и мы делим данные и отправляем на разные процессоры. Существенно сложнее создание таких систем.

![image_3.png](image_3.png)

Что произошло с момента когда появились параллельные системы. 
В отличие от последовательных систем, параллельные архитектуры стали проявлять 
большое разнообразие. Каждый следующий параллельный компьютер - 
это было что-то новое. И сложно писать ПО для таких систем. 
Первая попытка предпринята Флинном: классификация системы на 4 класса, 
выделив потоки команд и данных и предположил что каждый из этих потоков может быть одновременно или по отдельности одиночным или множественным.

![image_4.png](image_4.png)

Самая простая:

![image_5.png](image_5.png)

Программа выбирает одну команду, один операнд и есть одно исполнительное устройство.

![image_6.png](image_6.png)

МКОД - когда есть поток данных, из него выбирают один операнд и применяя к одному и тому же элементу данных одновременно параллельно разные инструкции.

![image_7.png](image_7.png)

Классический вариант - разбили данные на несколько частей, и применяем к каждому куску одни и те же команды. Но довольно не многие современные системы можно отнести к SIMD. Гораздо больше встречается другой подход SPMD. (Single program multiple data).  Отличаются одним моментом - нет синхноррного исполнения. В SIMD процессоры не просто выполняют одни и те же команды над одними и теми же данными, они еще и делают это одновременно. При таком подходе мы получаем SIMD.

![image_8.png](image_8.png)

90-99% ВС которые сейчас есть - MIMD. Главный минус классификации Флинна - современные системы вообще не классифицирует современные системы, относя все к одному классу - MIMD.

![image_9.png](image_9.png)

Это развитие - MIMD класс раскрыть и разделить на некоторые подклассы. Выделил конвейерные MIMD-системы, а все остальные - переключаемые и непереключаемые.

**Переключаемость** - это такие архитектуры где нет жесткого физического соединения между узлами, между процессами и памятью например. Множество процессоров и множество модулей памяти могут взаимодействовать между собой в любых комбинациях при помощи переключателей - некое устройство позволяющее соединять процессы и память.

Главный критерий разделения класса MIMD - системы с общей памятью и распределенной.

![image_10.png](image_10.png)

Довольно грубая количественная классификация

![image_11.png](image_11.png)

Разделяет ВС на 3 уровня, на каждом уровне введено по 2 числа
K - число процессоров
D - число АЛУ в одному процесоре
W - число бит

И k’, d’, w’ - все те устройства могут быть сцеплены в конвейер. То есть w’ означает что может быть конвейерная обработка бит к примеру.

![image_12.png](image_12.png)

Выделяет 4 вида компонентов.

![image_13.png](image_13.png)

Цель введенния классификаций - на какую конкретно систему наша программа ориентирована. Цели не достигнуты, потому что особенности архитектуры таковы что программа либо не учитывает особенности системы и работает неэффективно, а если и учитывает то она не заработает на другой архитектуре.


## 2. Архитектуры параллельных вычислительных систем. Развитие архитектур процессоров. Графические процессоры.

![image_14.png](image_14.png)

Современные массовые параллельные системы свелись к 4 основным типам архитектур, где мы можем конкретно выбрать конкретный принцип разработки.

![image_15.png](image_15.png)

Есть множество процессоров подключающиеся к общей шине, к которой подключается еще и память и одна доступа всем процессорам - поэтому она симметрична.

Хорошо тем что для такой архитектуры практически не нужно специальных подходов создания программ.

Недостаток - ограничение число Процессоров так как общая шина это просто не позволит, а вторая проблема техническая - наличие встроенного кэша без которого они работать не могут, может быть разное видение кэша

![image_16.png](image_16.png)

Здесь нет общей памяти, вся система - набор узлов в каждом из котором есть свой процессор и своей локальной памятью. сОЕДИНЕНы некой коммуникационной средой (комп сеть).

Хорошо - масштабируемость, можно построить систему из оч большо числа процессоров.

Плохо - трудоемкость и межпроцессорный обмен, ведь тут не просто шина а целая комп сеть - это все накладные расходы. А если мы память распределили то работа с большими массивами памяти может быть ограничена.

(90% современных вс - mpp)

![image_17.png](image_17.png)

Это компромисс - вроде бы берем систему с массово параллельной архитектурой с распределенной памятью и комп сетью, но при этом мы запускаем на системе некую специальную ОС (или доп системные средства) которые эмалируют для всех процессоров этой системы общую память.

Минус - можем по ошибке непреднамеренно запустить передачу данных по сети

Когенентность кэша здесь еще более острая проблема, нужно не просто сбросить кэш а еще и передавать данные по сети чтобы синхронизировать кэш


![image_18.png](image_18.png)

Раньше была популярной - первые суперкомпы строились на векторной-конвейерных вычислителях.

Принцип в том что мы ориентируемся на векторной-матричные операции и параллелизм достигаем за счет того что мы операции выполняем параллельно над всеми компонентами вектора.

![image_19.png](image_19.png)

Технически - маркетинговый термин, это некий компромисс между одноядерностью и многоядерностью, ведь многоядерность требует много ресурсов  кристалла.
А в intel увидели что функциональное устройство в каждом ядре процессора используется даже не на 90%, и сделали эффективнее - сделали так чтобы два логических процессора испо5двльзовали одну и ту же базу логических функциональных устройств.

Дает выигрыш почти в полтора ядра

![image_20.png](image_20.png)

Чистый smp на кристалле

Есть 2 уровня кэша L1, L2

L1 - личный кэш каждого процессора

L2 - обычно делится между ядрами и это дает эффективность

Решается проблема когенентности кэша так еще и позволяет меньше промахов делать в кэше

![image_21.png](image_21.png)

![image_22.png](image_22.png)

Во первых тут устройства делятся на блоки, каждый блок содержит внутреннюю память нескольких типов, (память текстур, и внутренняя память разделяемая к которой все процессоры и имеют доступ),

В основном эти процессоры хорошо работают с внутренней памятью

Вычисления строятся блоками

## 3. Производительность параллельных компьютеров. Пиковая и реальная производительность. Ускорение. Эффективность. Законы Амдала и Густавсона-Барсиса.

![image_23.png](image_23.png)

Есть разные подходы к оценке производительности

Щас используется число Вещественных операций в единицу времени. На практике ни в какой реальной программе не может быть так что в течение длительного времени вся программа это последовательность floating point operation.

![image_24.png](image_24.png)

Чем плох бенчмарк - показывает производительность программы вовсе не на той программе которую вы будете запускать

![image_25.png](image_25.png)

Если мы запустили на системе некую программу и научились мерить например количество операций которое каждое из вычислительных устройств дает в единицу времени, можем ввести еще одну характеристику - реальную производительность системы

Мы знаем сколько процессоров (s) в системе, мы знаем пиковую производительность из документации, и мы померили загруженность

Реальная пр-ть - сколько команд выполняет в секунду система

![image_26.png](image_26.png)

![image_27.png](image_27.png)

![image_28.png](image_28.png)

Закон амдала примиеняем тогда когда программа есть перед глазами и можно увидеть сколько команд последовательно а сколько параллельно выполняется и можем посчитать макс ускорение

А по этой формуле мы понимаем достигли ли мы предел или нет - распараллелили по максимуму или нет


## 4. Математические модели параллельных вычислений. Графовые модели программ.

![image_29.png](image_29.png)

Когда программу представляют графом, выделяются ребра (граф обычно ориентированный - и там дуги), вершины сопоставляются неким действиям программ.


Крупноблочные модели - там вершина может быть целая подпрограмма

Операторы могут быть разные и их два типа - преобразователи и распознователи

Преобразователи - большинство операторов, что-то делает с данными

Распознаватели - условный оператор

Самая простая модель - граф управления, ближе к структуре схемы алгоритма, где дуги - по сути последовательность выполнения операторов друг за другом

**Граф управления показывает все возможные переходы управления между операторами в программе**. (Статическая структура программы).
Динамическая развертка - операционно логическая история программы

![image_30.png](image_30.png)

**Информационный граф программы** - он определяет не передачу управления,  а передачу данных. В инфографе нет вершин распознавателей, потому что они по сути не работают с данными и не участвуют в инфообмене. А дуги в этом графе означают отношения инфозависимости - это возможность передачи информации.
Строить существенно сложнее, чем граф передачи управления, потому что не всегда инфосвязь видна. Инфограф можно развернуть, запустив программы и посмотрев фактические срабатывания - это история реализации программы (граф алгоритма).


Граф зависимостей  - примерно то же самое, только вместо отношения инфозависимости, которую не всегда очевидно можно понять, граф зависимостей строит отношения зависимости операторов. (Обращение к общим ячейкам памяти)

![image_31.png](image_31.png)

Основная задача построения графов - либо проанализировать некий алгоритм на предмет его распараллелизуемости, либо распараллелить алгоритм чтобы выполнить его на параллельной архитектуре определенного типа. По графу видно как распределить работу между множеством исполнительных устройств, а также как преобразовать программу и ее граф путем выполнения эквивалентных преобразований - изменить структуру графа, но не меняя результата выполнения программы.

Если в графе нет циклов то можно ввести порядок, то этот граф с разметкой называется строгой параллельной формой графа (строго потому что порядок соблюдается - i строго меньше j).

**Каноническая форма** - все входные вершины находятся в Группе с индексом 1, а все пути до вершины k, макс длина будет k-1, то есть граф пронумерован так что в этом графе мы не сможем построить избыточные пути с избыточной длиной

По параллельной форме графа можем построить анализ в определении яруса параллельной формы, то есть вершины которые имеют одинаковые индексы.

Среди всех параллельных форм можно найти те у которых есть минимальная высота, такие параллельные формы - максимальные параллельные формы, так как есть максимально широкий ярус.

**Все вершины одного яруса можно выполнить параллельно, на процессорах число которых равно ширине яруса.**

**Высота - это длина последовательных операций которые мы не можем распараллелить, чем больше высота - тем хуже.**

![image_32.png](image_32.png)

Чтобы соответствовать реальности задача сводится к более узкой задаче - анализу распараллеливаемости ЦИКЛОВ

Если в алгоритме нет циклов, то алгоритм и так достаточно быстро выполняться будет. Задача - определить какие циклы можно выполнить параллельно. А далее преобразовать программу.

**Для анализа распараллеливаемости циклов применяют инфографы**. (Граф алгоритма и 4 типа графов зависимостей, 4 типа тк если рассматриваем 2 операции - один источник данных, другой приемник данных, то может быть 4 разных комбинации зависимостей).

Out-in - первый оператор передает выходной результат в качестве входного параметра следующего оператора

In-out - входной параметр первого оператора является выходным второго

Out-out - оба оператора записывают результат в одну и ту же переменную

In-in - читают с одной и той же переменной

![image_33.png](image_33.png)

Простыми словами - между итерациями циклов не должно быть никакой инфозависимости - и тогда все итерации цикла можно выполнить одновременно (в идеале)

![image_34.png](image_34.png)

Последовательность суммирования позволяет сделать распараллеливание, в случае умножения матриц:

1) устанавливаем нули для матрицы

2) Затем выполняем k шагов по индексу k, на каждом шаге берем предыдущие значения a_ij , и прибавляем к нему произведение эл-ов матрицы

Получили 3 вложенных цикла (для матриц размерностью n = 2), поэтому в пространстве итераций цикла содержит 4 вершины

На такой картинке очевиден параллелизм - мы должны выделить такие группы итераций которые не связаны дугами - видно что верхние 4 вершины не связаны и нижние не связаны, а нижние с верхними есть инфозависимость , это означает что по индексу k наш цикл не распараллеливаем, а по i и j никаких дуг нет. Значит мы можем взять цикл и выполнить последовательно n операций по k,  а по i и j можно n^2 операций на 4 процессорах выполнить параллельно, и высота нашего графа равна двум.

![image_35.png](image_35.png)

Здесь 4 яруса, между ними есть инфозависимость, но на каждом ярусе все вершины не имеют инфосвязи, таким образом параллелизм в том что мы выполним задачу в самом быстро случае за 4 шага, на каждом из шагов будет разное кол-во итераций, на 1 шаге будет 4 итерации (4 процессора), на 2-ом шаге 3, на последнем шаге один процессор выполнит последнее вычисление

Распараллеливание по индексу i, по j - не распараллеливается


## 5. Математические модели параллельных вычислений. Модели PRAM, BSP, LogP.

![image_36.png](image_36.png)

Анализ программы на детальном уровне - сложная задача, ведь в реальной программе пространство операторов может быть многомерной, наглядность теряется.

Модели параллельных вычислений (не программы) - более высокоуровневые. Это модель одновременно и задает правило оформления программы, но в то же время эта модель моделирует вычислительную систему.

![image_37.png](image_37.png)

Есть n процессоров симметричных, есть общая память с одинаковым доступом и одинаковой скоростью, и есть центральное управляющее устройство - тактовый генератор

![image_38.png](image_38.png)

То есть в pram процессоры строго одновременно двигаются по этой программе log-step и выполняют одновременно команды

В каждом цикле каждая команда выполняется всеми процессорами одновременно, в том числе если команда состоит из нескольких считываний и прочих операций.

Более того если в цикле есть условные операторы, то они также выполняются синхронно.

![image_39.png](image_39.png)

То как процессоры работают с память. Память общая и имеет неограниченное кол-во каналов доступа. При этом каждая команда состоит из 3-ех фаз - чтение операнда, выполнение операции и запись результата. И процессоры будут делать это одновременно.

EREW - два процесса и более не могут одновременно считать одну и ту же ячейку памяти или записать, в этой модели это ошибка и программа некорректна, но к разным могут одновременно обращаться

CREW - читать можем всеми процессорами одновременно, но писать в одну ячейку не можем

ERCW - почему-то читать не можем но писать можем конкурентно

CRCW - полностью конкурентная модель. Писать можно одновременно, но что именно писаться будет - модель не регламентирует

Чаще всего алгоритм создается одной из двух первых моделей - EREW/CREW.

![image_40.png](image_40.png)

Сначала нужно переменную x всем процессорам размножить, продублировав в общей памяти чтобы процессоры могли эту переменную за один такт считать не конкурируя друг с другом

Логарифм потому что на каждом шаге в два раза больше переменных копируется

Если n - степень двойки , то k итераций выполняем, и на каждой итерации выполняем параллельный цикл, почему они могут вып одновременно - потому что между итерациями цикла нет инфозависимости

![image_41.png](image_41.png)

Задача решается методом сдваивания, на каждом шаге мы находим минимумы двух соседних элементов массива, записываем в тот же массив для того чтобы не было лишнего копирования, на 1 шаге мы используем n/2 процессоров , затем еще в два раза меньше, затем еще в два раза меньше и так далее мы получаем исходный элемент

Нюанс для модели PRAM: мы должны результаты вычислений поместить в начало массива затерев исходные данные массива (последняя итерация параллельного цикла), и используется свойство модели PRAM - j-ый элемент массива используется выше на нескольких итерациях, и если бы это была не модель PRAM, а просто программа на С, то здесь могла быть проблема. Мы точно знаем что все процессоры сначала зайдут в if, потом операцию внутри if, а те которые не выполняют - ее подождут


![image_42.png](image_42.png)

Сложность О(n) для последовательной реализации

Есть прямая зависимость между итерациями - сделать распараллеливание нельзя, в лоб не решить

![image_43.png](image_43.png)

Бьем массив пополам, (пусть четное число эл-ов), с первого по n/2 решаем последовательным способом и получаем результат, потом берем вторую часть массива и точно также последовательно решаем задачу, получилось два куска, и чтобы получить конечное решение нужно просто взять самый последний элемент из первой половины и умножить на все элементы второй половины - n/2 умножений, но теперь шаги 1 и 2 можно распараллелить, а также третью операцию умножения можно выполнить параллельно

Получили ускорение в 2 раза

Идея в том что каждую из половин можно ЕЩЕ разделить и так далее

![image_44.png](image_44.png)

Сверху 8 элементов массива 1 - 8

Дальше мы умножаем 1й на 2й, 5й на 6й и 7й на 8й, потом умножаем А12 на А3, получаем А13 и так далее , получили что у нас есть 3 яруса, на 1 ярусе мб задействованы 4 процессора, на 2ом ярусе 2 процессора и на последнем один процессор

Граф немного скошенный - и работа по процессорам распределена не очень равномерно

После первого яруса часть процессоров простаивает

![image_45.png](image_45.png)

Этот граф тоже самое делает но есть еще и одно преимущество - элементы данных четко привязаны к процессору, элементы 1 и 8 исп-ся только на первом процессоре и так далее

Все 4 процессора работают все время

![image_46.png](image_46.png)

Глубина логарифмическая поэтому внешний цикл по k
Внутри два вложенных цикла и при этом что каждый из циклов параллелен и можем рассматривать все их пространство итераций (квадрат) как одно общее пространство итераций и распараллеливать два вложенных цикла одновременно

Поэтому можем сразу посчитать все итерации на каждом шаге k все количество итераций вложенных циклов и все итерации распределить по процессорам

Почему crew pram - потому что здесь есть одно место где мы считываем элемент массива a, и здесь будет появляться конкурентное чтение между процессорами - это норм для CREW.

Для модели EREW нужно будет построить другой граф

![image_47.png](image_47.png)

Здесь никогда один и тот же элемент данных на каждом ярусе алгоритма не считывается одновременно более чем один процессора

![image_48.png](image_48.png)

![image_49.png](image_49.png)

Идея в том что взять модель pram но заложить более реалистичный учет затрат на передачу данных между процессорами

В этой модели есть множество процессоров но они обладают собственной памятью и есть коммуникационная сеть и средство синхронизации процессоров

Модель также синхронная - процессоры одновременно выполняют свои действия, и они синхронизируется при помощи барьерной синхронизации

![image_50.png](image_50.png)

Если в модели PRAM четкость синхронизации по шагам, по сути по каждому оператору, то в bsp вводится понятие супершага, в котором множество команд

Он состоит из трех фаз

1) фаза вычислений - все процессоры независимо что то вычисляют

2) Фаза обмена - все процессоры завершают (дожидаются) и обмениваются данными

3) В конце барьерная синхронизация - все процессоры ее проходят

Следующая модель вычисляет затраты на обмен через сеть

![image_51.png](image_51.png)

Допущение - то что сообщения одинаковые

Барьерная синхронизация считается мгновенной

![image_52.png](image_52.png)

В модели pram этот алгоритм использовался чтобы при отсутствии конкурентного чтения мы могли изначально распространить данные по всем процессорам

Бродкаст в BSP еще важнее так как нет общей памяти

![image_53.png](image_53.png)

LogP - еще ближе к системам с распределенной памятью и обмениваются процессоры с другими тогда когда им удобно. Поэтому вводятся доп параметры

![image_54.png](image_54.png)

Процессоры действуют также как в модели BSP, но в отличие от BSP процессоры ничего не ждут - они как только получили данные сразу готовы их отправлять но они должны подождать overhead, потому есть асинхронность

Дерево зависит не только от алгоритмов но и от L, o, g.


## 6. Техники построения параллельных алгоритмов. Разделяй и властвуй (Divide-and-conquer). Рандомизация. Прыгающий указатель (Pointer jumping).

![image_55.png](image_55.png)

Второй класс техник построения алгоритмов - рандомизация

![image_56.png](image_56.png)

Введення некой искусственной случайности в алгоритм
Рандомизация позволяет найти параллелизм там где его изначально не видно

Самый простой способ - выборка, где мы решаем задачу выбирая некую часть и решая на ней задачу


Нарушение симметрии - исходные данные могут быть симметричны, и при распараллеливании это играет отрицательную роль, пример - поиск группы независимых вершин в графе, то есть вершин которые между собой никак не связаны ни одной дугой

Если граф близок к решетке, то мы бы параллельно взяли каждую вершину и аналазировали, и у нас были бы конфликты по данным

За счет рандомизация мы можем взять подмножество вершин, граф потеряет симметричный вид и мы начнем параллельно решать

Также еще есть проблема того что задача может распараллеливаться неплохо, но количество процессоров на каждом шаге разное, например в броадкасте. За счет балансировки нагрузки мы если отвязываем прямую привязку иттераций цикла.

![image_57.png](image_57.png)

Третий тип техник. На таких структурах часто решаются разные задачи и хочется их распараллелить. Структуры хороши, но минус в том что если мы берем связанный список то просто трудно анализировать его параллельно , потому есть техники:

1) pointing jumping - чтобы пробежаться по связанной структуре берем все вершины этой структуры и на каждом шаге алгоритма одновременно для всех узлов заменяем его указатель на следующий узел через уровень (прыжок)
2) Прочие
3) И прочие


## 7. Параллельные алгоритмы в модели PRAM. Broadcast. Поиск элемента массива. Поиск минимума в массиве. Сумма элементов массива. Префиксная сумма.

![image_58.png](image_58.png)

Здесь берем пары элементов и решаем задачу для пар элементов, массив уменьшается в два раза и повторяем задачу до тех пор пока весь массив не схлопнем в одно число

Здесь алгоритм написан для модели PRAM.

Почему сложность O(n) - с одной стороны у нас цикл верхний по логарифму n, а второй по n / pow2, и за счет того что каждый раз делим на 2 - суммарное количество все равно будет O(n). Очевидно что последовательно тоже O(n). Получили такую же выч сложность.

Высота - число шагов которые нужно выполнить одним параллельным процессором. Эта величина определяет конечное время решения алгоритма на параллельной системе с неограниченным числом процессоров и определяет ускорение за счет распараллеливания.

Число Процессоров n/2 так как Параллельная часть начинается с n/2 и потом меньше и меньше - изначально нужно предоставить параллельную систему с n/2 процессоров.


![image_59.png](image_59.png)

С высотой просто, внешний последовательный цикл и внутренний параллельный.

Но со сложностью не так просто - у нас log n шагов, но на каждом шаге выполняем число итераций которое сначала равняется n-1, потом n-2, … и до 1. В итоге получили Выч сложность O(n log n). Проиграли по сравнению с последовательной реализацией -

![image_60.png](image_60.png)

![image_61.png](image_61.png)


Делим массив пополам, вычисляем префик сумму для пар , первый параллельный цикл по исходному массиву считает произведение всех пар, и записывает в новый массив, а далее рекурсивно вызываем тот же алгоритм для массива а, записывая в z.

Далее собираем результат, merge выполняется параллельно - n параллельных операций

![image_62.png](image_62.png)

Зачем сделали так если сложность такая же - решив за то же время но с меньшим числом вычислительных операций мы сэкономим ресурсы кластера или большим решим задач одновременно на одном и том же кластере


## 8. Параллельные алгоритмы в модели PRAM. Прыгающий указатель (Pointer jumping). Ранжирование списка. Эйлеров обход дерева.

![image_63.png](image_63.png)

Есть связанная структура данных - successor это указатель на следующий элемент

Для каждого элемента структуры должны построить указатель на конечный элемент

В модели crew pram - сначала синхронно считаются всеми процессорами successor[i], а потом синхронно запишутся эти данные в соответствующие указатели каждого элемента - никакого конфликта по записи не будет, но по чтению будут конфликты поэтому задача решается для CREW PRAM.

![image_64.png](image_64.png)

![image_65.png](image_65.png)

На реалистичной машине нет четкой синхронизации по шагам, весьма вероятно что отдельные процессоры могут мешать друг другу пытаясь записать одновременно в один и тот же элемент successor[i]

Для разрыва конкурентного доступа может потребоваться введение доп массива, который рассчитывается на первом шаге параллельно всеми процессорами до того как обновлены значения, затем реализуется барьер между процессорами и после этого смело присваиваем значения successor[i].

![image_66.png](image_66.png)

Задача - помимо того чтобы переназначить указатели в каждой вершине на конец списка, еще и расстояние до корня посчитать для каждой вершине

![image_67.png](image_67.png)

![image_68.png](image_68.png)


## 9. Параллельная сортировка. Чётно-нечётная сортировка.

![image_69.png](image_69.png)

На каждом шаге сортируем либо только четные элементы либо только нечетные

И на каждом шаге мы можем делать сравнения независимо пар элементов так как они не пересекаются


![image_70.png](image_70.png)

В цикле каждый процессор если он четный и элемент четный, то получает элемент от соседнего процессора справа и отправляет ему свой
После чего оставляет у себя минимум

Если процессор нечетный то он обменивается процессором слева

За счет обменов и сравнений процессоры решают задачу параллельно, но общее число сравнений все равно квадрат, а высота алгоритма уже O(n), то есть можно решить задачу сортировки за линейное время на n процессорах

Возможно это не все!!!

## 10. Параллельная сортировка. Параллельный QuickSort.

![image_71.png](image_71.png)

Все упирается в функцию разделения - разбиение массива на 2 (алгоритм Хора)

Берем в качестве pivot середину массива и дальше бежим с двух сторон - с нижнего индекса массива i и верхнего j. До тех пор пока мы встречаем элементы меньше pivota растет i и наоборот для j

Когда наткнулись на неупорядоченные элементы - переставляем два элемента и так до тех пор пока i != j

Не будем распараллеливать partition(), а распараллелим все остальное

![image_72.png](image_72.png)


Сама процедура partition() - линейная пропорционально числу шагов, но на каждом шаге массив уменьшается в два раза

Что нам дало - O(n) - при удачном выборе пивота

Что было - O(n log n)

Проблема здесь последовательного partition() - поэтому и ускорение не очень получили

Один из подходов (теоретический) -

![image_73.png](image_73.png)

Предложена фиктивная схема алгоритма partition(). Алгоритм использует конкурентной записи - arbitrary concurrent write, суть в том что когда записываем одновременно в один и тот же элемент памяти - но результат записи случайный. Но чтение работает без конфликтов. Разделение массива работает как дерево и алгоритм подходит к задаче как к построению дерева разбиения массива на части.

N здесь - число процессоров

На практике есть некоторое приближение к этой модели

![image_74.png](image_74.png)

![image_75.png](image_75.png)


## 11. Параллельная сортировка. Параллельный Merge Sort. Odd-Even MergeSort.

Это просто жопа

![image_76.png](image_76.png)

![image_77.png](image_77.png)

Сложность осталась такойже но высота стала O(n) - всего лишь в log раз ускорение что недостаточно и все упирается в merge - его нужно распараллелить

![image_78.png](image_78.png)

![image_79.png](image_79.png)


## 12. Технологии многопоточного программирования. Pthreads.

![image_80.png](image_80.png)

Подход - отказ от разделения памяти - потоки всегда работают внутри одного процесса - эффективность обмена данными повышается

Что роднит поток и процесс - у потока есть свой контекст, контекст состоит из - id потока, указатель текущей команды (потому что в момент переключения потока мы должны знать с какого места продолжить выполнения потока), и у каждого потока есть свой стек и стек тоже переключается, и содержимое регистра процессора


Переключение потока гораздо быстрее чем переключение процесса

Переключение - это про запись состояния контекста потока и есть два режима

Режим ядра - Защищенный режим Работы процесора при котором происходит переключение, это хорошо так как позволяет полностью вытеснять потоки ОС независимо от того хочет этого поток или нет

Пользовательский режим - потоки Реализуются не как Процессы а как какие-то легковесные структуры в рамках одного процесса. Ядро ОС ничего не знает об этих потоках.

![image_81.png](image_81.png)

![image_82.png](image_82.png)

Любая программа - всегда содержит один главный поток даже если не используем многопоточность

Главный поток порождает дочерние и так далее получаем дерево неограниченной глубины

![image_83.png](image_83.png)

Есть три варианта завершения потока - первый самый правильный


Потоки бывают detached и обычные, обычный поток - он сохраняет связь с родительским потоком и родительский может сделать join, join ожидает завершения потока. Detached не привязывается к родительскому и нельзя сделать join, но зато когда поток завершится - он автоматически освободит ресурсы

![image_84.png](image_84.png)

Функция возвращает int, если вернула 0 - поток создан и уже начал выполняться, либо отрицательное число - тип ошибки

![image_85.png](image_85.png)

![image_86.png](image_86.png)

![image_87.png](image_87.png)

Проблема кода примера 4 - после того как мейн сделал detach() функции нужно что-то продолжить делать и может быть такое что процесс завершится путем завершения main раньше чем создадутся все потоки!

И все потому что join сделать мы не можем

![image_88.png](image_88.png)

Мьютексы бывают нескольких типов, это влияет на контроль состояния мьютекса со стороны библиотеки

Условие корректной работы мьютекса - сделать lock и unclock

Normal - он ничего не контролирует

errсheck - будут ошибки если попытаемся открыть мьютекс который закрывали не мы

Recursive - на слайдах далее

![image_89.png](image_89.png)

![image_90.png](image_90.png)

Недетерминированное поведение поскольку после pthread_create непонятно как скоро этот поток будет запущен

![image_91.png](image_91.png)

Здесь детерминированно

Здесь mutex_lock до pthread_create, и даже если pthread_create запустит наш поток сразу, то все равно наша функция потока f на мьютексе гарантированно засыпает, поэтому присваиваем b до того как начинают работать потоки, потом делаем join. A = 1, b = 0.

![image_92.png](image_92.png)

Можем получить a и b вообще в разных комбинациях, но блокировка работает

![image_93.png](image_93.png)

Чтение можно сделать конкурентным чаще всего, потому что чтение общей переменной в общем случае операция корректная и детерминированная и за счет этого можно повысить производительность системы если не будем блокировать несколько потоков которые читают переменные и устанавливать блокировку будем только на момент записи

Логика следующая

1) пока нет ни одной блокировки на запись - можно сколько угодно установить блокировок на чтение
2) Пока есть хотя бы одна блокировка на чтение - нельзя установить блокировку на запись
3) Пока есть блокировка на запись - нельзя установить на чтение

![image_94.png](image_94.png)

![image_95.png](image_95.png)

Объект похожий на мьютекст но в отличие от мьютекса у семафора есть еще и внутреннее значение (целое)

![image_96.png](image_96.png)

Семафором могут пользоваться разные потоки в отличие от мютекса

Другой поток может увеличивать семафор а другой тупо ждать

![image_97.png](image_97.png)

Детерминировано

Создаем семафор с нулевым значением, создаем поток и блокируемся на семафоре , и как только мы вышли из sem_wait гарантированно a увеличилась на 1, потому что только после этого f сделаем sem_post. Выход - 1,1

![image_98.png](image_98.png)

Нет детерминизма.

Можем не дождаться пока функция f выполнится ,  мы можем как успеть выполнить операцию sem_post так другим потоком так может и не успеть

![image_99.png](image_99.png)

В случае sem_trywait мы не блокируемся, проверяем значение семафора и если оно ненулевое то уменьшаем, и дальше проходим

Во втором примере получаем значение семафора getvalue, если ненулевое то с помощью sem_wait уменьшаем значение семафора и продолжаем

В каком то смысле коды делают одно и то же, но эквивалентны ли они? Нет, trywait атомарная, а во втором случае неатомарна!

![image_100.png](image_100.png)

Это некий способ нотификации об изменениях состояния, на этом примитиве синхронизации можно ожидать и нотифицировать о неком событии

Три основные функции wait, signal, broadcast

wait - Нам нужно создать некий мьютекст который будет связан с этой переменной, это важно потому что именно этот метод обеспечивает атомарную работу переменной, мьютекст должен быть залочен. Можно например проверять значение разделяемой переменной и если нам состояние переменной не устраивает мы входим в ожидание и внутри ожидания атомарно мьютекст анлочится.

Signal, broadcast - сигнализирует

![image_101.png](image_101.png)

![image_102.png](image_102.png)

![image_103.png](image_103.png)

Еще один примитив синхронизации

Барьер может только ждать, мы указываем value - число участников барьера

Когда число потоков value к барьеру подойдет - они все вместе пройдут барьер а до этого будут засыпать перед барьером (лочиться)

![image_104.png](image_104.png)

Создали барьер, запустили три потока, у всех одна функция потока, функция увеличивает счетчик и ждет барьер. Если запустим программу, мы увидим что каждый поток печатает значение разделяемой переменной, очевидно 3 раза напечатается число 3 (в предположении что  a++ атомарно - в коде ошибка)

Детерминированность в том что все потоки получат одинаковое значение

![image_105.png](image_105.png)

Еще одна конструкция - pthread_once_t - выглядит как создание нового потока, принимается указатель на ф-ю, объект который получается превращается в pthread_once, но в отличие от запуска потока, функция может быть вызвана несколькими потоками, но вызовется она ровно один раз

![image_106.png](image_106.png)

![image_107.png](image_107.png)

Эта конструкция не связана с синхронизацией. Эквивалентно локальной переменной потока. В каждом потоке свое значение этой переменной

![image_108.png](image_108.png)


## 13. Технологии многопоточного программирования. C++ Concurrency.

![image_109.png](image_109.png)

![image_110.png](image_110.png)

Недетерминированный вывод программы так как порядок вызова join не гарантирует порядок вывода - второй поток может опередить первый

![image_111.png](image_111.png)


Можно вызвать внутри потока и оперировать данным потоком, например заснуть

![image_112.png](image_112.png)

![image_113.png](image_113.png)

Можно легко забыть залочиться или заанлочиться тем более, поэтому в С++ есть lock_guard - обертка над мутексом

![image_114.png](image_114.png)

![image_115.png](image_115.png)

Плюс - не рабоатет планировщика и переключения контекста

минус - наш поток продолжает тратить такты процессора в while

![image_116.png](image_116.png)

compare_exchange_strong - если tst_val и ai совпали то ai меняется на new_val, если не совпали то newVal меняется местами с tst_val.



## 14. Технологии многопоточного программирования. OpenMP.

![image_117.png](image_117.png)

![image_118.png](image_118.png)

(Spmd - разновидность mimd-а)

![image_119.png](image_119.png)

![image_120.png](image_120.png)

Компилятор после завершения структурированного блока вставит барьер синхронизации чтобы все потоки друг друга ждали, и из выхода из parallel директивы мы опять остаемся в мастер треде.


![image_121.png](image_121.png)

На самом деле в блоке parallel создаются не потоки, а в этот момент создается множество виртуальных задач

Дальше эти задачи начинают управляться openmp и он распределяет эти задачи потокам

![image_122.png](image_122.png)

![image_123.png](image_123.png)

Память общая - openmp принципиально создавалась для систем с общей памятью

Все таски имеют доступ к общей памяти

![image_124.png](image_124.png)

OpenMP потенциально ориентирована на работу с системами numa - с неоднородным доступом

В этой системе каждый процессор имеет свою локальную копию общих данных которые могут в некоторые моменты времени и не совпадают друг с другом

Расслабленная модель согласованности - в общем случае openmp не гарантирует что для разделяемых переменных будет одно и то же значение

Мы должны флашить - мы подтягиваем изменения в локальную память тогда когда нам это нужно - это и есть relaxed consistency model

![image_125.png](image_125.png)

ПРОПУЩЕНО НЕСКОЛЬКО СЛАЙДОВ ПО OpenMP!


## 15. Технологии программирования на основе параллельных процессов. Средства межпроцессного взаимодействия POSIX.
## 16. Технологии программирования на основе параллельных процессов. Интерфейс передачи сообщений (MPI). Обмен сообщениями между процессами. Группы процессов. Коммуникаторы.
## 17. Технологии программирования на основе параллельных процессов. Интерфейс передачи сообщений (MPI). Коллективные обмены. Удаленный доступ к памяти. Управление процессами в MPI.
## 18. Блокирующая и неблокирующая синхронизация. Уровни неблокирующей синхронизации. Атомарные операции Fetch and Add, Compare and Swap (CAS), Load-Link/Store-Conditional.
## 19. Проблема ABA. Решения проблемы ABA. Освобождение памяти в lock-free алгоритмах.
##  20. Lock-free алгоритмы. Алгоритмы для стека, очередей FIFO. Алгоритмы MS-Queue, LCRQ.
## 21. Lock-free алгоритмы. Lock-free list-based set, lock-free hash table, lock-free алгоритм для деревьев.
## 22. Wait-free алгоритмы. Методология “быстрый-путь-медленный-путь”. Wait-free queue.
## 23. Проблемы параллельного доступа к общей памяти. Когерентность и согласованность общей памяти. Модели согласованности памяти. Последовательная согласованность.
## 24. Модели согласованности памяти. Расслабленные модели согласованности (Relaxed consistency). Processor Consistency (PC). Total Store Ordering (TSO). Weak ordering (WO). Release Consistency (RC).
## 25. Барьеры памяти. Согласованность памяти и компиляторы. Синхронизированные операции в Java Memory Model (JMM).
## 26. Явное управление доступом к памяти в Java. Класс java.lang.invoke.VarHandle. Режимы памяти в java.util.concurrent.atomic. Явное управление порядком доступа к памяти в C++.

